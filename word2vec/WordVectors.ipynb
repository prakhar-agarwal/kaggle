{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk.data\n",
    "import numpy as np\n",
    "# Read data from files\n",
    "\n",
    "train = pd.read_csv(\"labeledTrainData.tsv\",header=0,delimiter=\"\\t\",quoting=3)\n",
    "test = pd.read_csv(\"testData.tsv\",header=0,delimiter=\"\\t\",quoting=3)\n",
    "unlabeled_train = pd.read_csv(\"unlabeledTrainData.tsv\",header=0,delimiter=\"\\t\",quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify the number of reviews that were read (100,000 in total)\n",
    "print \"Read %d labeled train reviews, %d labeled test reviews, \" \"and %d unlabeled reviews\\n\" % (train[\"review\"].size,test[\"review\"].size, unlabeled_train[\"review\"].size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import various modules for string cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    #review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer\n",
    "# NLTK's punkt tokenizer for sentence splitting. \n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    review=review.decode(\"utf8\")\n",
    "    review = BeautifulSoup(review).get_text()\n",
    "    review = review.lower()\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n",
      "Parsing sentences from unlabeled set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/p0a0045/anaconda/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://www.archive.org/details/lovefromastranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/Users/p0a0045/anaconda/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://www.loosechangeguide.com/loosechangeguide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/Users/p0a0045/anaconda/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://www.youtube.com/watch?v=a0ksqelmgn8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print \"Parsing sentences from training set\"\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "\n",
    "print \"Parsing sentences from unlabeled set\"\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "788128\n"
     ]
    }
   ],
   "source": [
    "print len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/p0a0045/anaconda/lib/python2.7/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n",
      "2016-11-09 20:49:17,844 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2016-11-09 20:49:17,980 : INFO : collecting all words and their counts\n",
      "2016-11-09 20:49:17,980 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2016-11-09 20:49:18,097 : INFO : PROGRESS: at sentence #10000, processed 220935 words, keeping 32824 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-11-09 20:49:18,222 : INFO : PROGRESS: at sentence #20000, processed 442235 words, keeping 51884 word types\n",
      "2016-11-09 20:49:18,341 : INFO : PROGRESS: at sentence #30000, processed 658732 words, keeping 67452 word types\n",
      "2016-11-09 20:49:18,472 : INFO : PROGRESS: at sentence #40000, processed 880055 words, keeping 81578 word types\n",
      "2016-11-09 20:49:18,597 : INFO : PROGRESS: at sentence #50000, processed 1093476 words, keeping 93998 word types\n",
      "2016-11-09 20:49:18,711 : INFO : PROGRESS: at sentence #60000, processed 1310274 words, keeping 105391 word types\n",
      "2016-11-09 20:49:18,844 : INFO : PROGRESS: at sentence #70000, processed 1530308 words, keeping 116352 word types\n",
      "2016-11-09 20:49:18,965 : INFO : PROGRESS: at sentence #80000, processed 1744691 words, keeping 126462 word types\n",
      "2016-11-09 20:49:19,085 : INFO : PROGRESS: at sentence #90000, processed 1961880 words, keeping 136857 word types\n",
      "2016-11-09 20:49:19,203 : INFO : PROGRESS: at sentence #100000, processed 2179175 words, keeping 146410 word types\n",
      "2016-11-09 20:49:19,328 : INFO : PROGRESS: at sentence #110000, processed 2394423 words, keeping 155668 word types\n",
      "2016-11-09 20:49:19,452 : INFO : PROGRESS: at sentence #120000, processed 2613558 words, keeping 164873 word types\n",
      "2016-11-09 20:49:19,568 : INFO : PROGRESS: at sentence #130000, processed 2825491 words, keeping 173604 word types\n",
      "2016-11-09 20:49:19,700 : INFO : PROGRESS: at sentence #140000, processed 3042979 words, keeping 181719 word types\n",
      "2016-11-09 20:49:19,822 : INFO : PROGRESS: at sentence #150000, processed 3262070 words, keeping 190085 word types\n",
      "2016-11-09 20:49:19,944 : INFO : PROGRESS: at sentence #160000, processed 3480742 words, keeping 198310 word types\n",
      "2016-11-09 20:49:20,072 : INFO : PROGRESS: at sentence #170000, processed 3698450 words, keeping 206172 word types\n",
      "2016-11-09 20:49:20,197 : INFO : PROGRESS: at sentence #180000, processed 3915472 words, keeping 213647 word types\n",
      "2016-11-09 20:49:20,318 : INFO : PROGRESS: at sentence #190000, processed 4137090 words, keeping 221427 word types\n",
      "2016-11-09 20:49:20,445 : INFO : PROGRESS: at sentence #200000, processed 4354534 words, keeping 228823 word types\n",
      "2016-11-09 20:49:20,567 : INFO : PROGRESS: at sentence #210000, processed 4573512 words, keeping 236321 word types\n",
      "2016-11-09 20:49:20,694 : INFO : PROGRESS: at sentence #220000, processed 4794013 words, keeping 243975 word types\n",
      "2016-11-09 20:49:20,820 : INFO : PROGRESS: at sentence #230000, processed 5012866 words, keeping 251083 word types\n",
      "2016-11-09 20:49:20,942 : INFO : PROGRESS: at sentence #240000, processed 5231829 words, keeping 258131 word types\n",
      "2016-11-09 20:49:21,068 : INFO : PROGRESS: at sentence #250000, processed 5439586 words, keeping 264871 word types\n",
      "2016-11-09 20:49:21,193 : INFO : PROGRESS: at sentence #260000, processed 5657181 words, keeping 271746 word types\n",
      "2016-11-09 20:49:21,319 : INFO : PROGRESS: at sentence #270000, processed 5874815 words, keeping 278770 word types\n",
      "2016-11-09 20:49:21,458 : INFO : PROGRESS: at sentence #280000, processed 6096159 words, keeping 286059 word types\n",
      "2016-11-09 20:49:21,596 : INFO : PROGRESS: at sentence #290000, processed 6314688 words, keeping 293158 word types\n",
      "2016-11-09 20:49:21,725 : INFO : PROGRESS: at sentence #300000, processed 6536710 words, keeping 300476 word types\n",
      "2016-11-09 20:49:21,850 : INFO : PROGRESS: at sentence #310000, processed 6754121 words, keeping 307254 word types\n",
      "2016-11-09 20:49:21,966 : INFO : PROGRESS: at sentence #320000, processed 6976023 words, keeping 314236 word types\n",
      "2016-11-09 20:49:22,100 : INFO : PROGRESS: at sentence #330000, processed 7196222 words, keeping 321087 word types\n",
      "2016-11-09 20:49:22,220 : INFO : PROGRESS: at sentence #340000, processed 7417920 words, keeping 328033 word types\n",
      "2016-11-09 20:49:22,340 : INFO : PROGRESS: at sentence #350000, processed 7635696 words, keeping 334528 word types\n",
      "2016-11-09 20:49:22,474 : INFO : PROGRESS: at sentence #360000, processed 7856824 words, keeping 340948 word types\n",
      "2016-11-09 20:49:22,602 : INFO : PROGRESS: at sentence #370000, processed 8079321 words, keeping 347741 word types\n",
      "2016-11-09 20:49:22,758 : INFO : PROGRESS: at sentence #380000, processed 8299353 words, keeping 354241 word types\n",
      "2016-11-09 20:49:22,883 : INFO : PROGRESS: at sentence #390000, processed 8519925 words, keeping 360541 word types\n",
      "2016-11-09 20:49:23,004 : INFO : PROGRESS: at sentence #400000, processed 8736910 words, keeping 366745 word types\n",
      "2016-11-09 20:49:23,134 : INFO : PROGRESS: at sentence #410000, processed 8953111 words, keeping 372640 word types\n",
      "2016-11-09 20:49:23,270 : INFO : PROGRESS: at sentence #420000, processed 9173169 words, keeping 379079 word types\n",
      "2016-11-09 20:49:23,403 : INFO : PROGRESS: at sentence #430000, processed 9395517 words, keeping 385290 word types\n",
      "2016-11-09 20:49:23,532 : INFO : PROGRESS: at sentence #440000, processed 9613851 words, keeping 391402 word types\n",
      "2016-11-09 20:49:23,669 : INFO : PROGRESS: at sentence #450000, processed 9836820 words, keeping 397659 word types\n",
      "2016-11-09 20:49:23,808 : INFO : PROGRESS: at sentence #460000, processed 10063270 words, keeping 404070 word types\n",
      "2016-11-09 20:49:23,940 : INFO : PROGRESS: at sentence #470000, processed 10287395 words, keeping 409868 word types\n",
      "2016-11-09 20:49:24,077 : INFO : PROGRESS: at sentence #480000, processed 10505928 words, keeping 415886 word types\n",
      "2016-11-09 20:49:24,205 : INFO : PROGRESS: at sentence #490000, processed 10726308 words, keeping 421993 word types\n",
      "2016-11-09 20:49:24,330 : INFO : PROGRESS: at sentence #500000, processed 10940552 words, keeping 427691 word types\n",
      "2016-11-09 20:49:24,463 : INFO : PROGRESS: at sentence #510000, processed 11160228 words, keeping 433493 word types\n",
      "2016-11-09 20:49:24,593 : INFO : PROGRESS: at sentence #520000, processed 11383128 words, keeping 439512 word types\n",
      "2016-11-09 20:49:24,725 : INFO : PROGRESS: at sentence #530000, processed 11603142 words, keeping 445404 word types\n",
      "2016-11-09 20:49:24,855 : INFO : PROGRESS: at sentence #540000, processed 11816581 words, keeping 451041 word types\n",
      "2016-11-09 20:49:24,992 : INFO : PROGRESS: at sentence #550000, processed 12040649 words, keeping 456791 word types\n",
      "2016-11-09 20:49:25,130 : INFO : PROGRESS: at sentence #560000, processed 12260439 words, keeping 462259 word types\n",
      "2016-11-09 20:49:25,261 : INFO : PROGRESS: at sentence #570000, processed 12481102 words, keeping 468035 word types\n",
      "2016-11-09 20:49:25,382 : INFO : PROGRESS: at sentence #580000, processed 12699166 words, keeping 473515 word types\n",
      "2016-11-09 20:49:25,516 : INFO : PROGRESS: at sentence #590000, processed 12918570 words, keeping 478820 word types\n",
      "2016-11-09 20:49:25,649 : INFO : PROGRESS: at sentence #600000, processed 13134666 words, keeping 484248 word types\n",
      "2016-11-09 20:49:25,778 : INFO : PROGRESS: at sentence #610000, processed 13355834 words, keeping 489702 word types\n",
      "2016-11-09 20:49:25,901 : INFO : PROGRESS: at sentence #620000, processed 13573828 words, keeping 494992 word types\n",
      "2016-11-09 20:49:26,024 : INFO : PROGRESS: at sentence #630000, processed 13791431 words, keeping 500253 word types\n",
      "2016-11-09 20:49:26,166 : INFO : PROGRESS: at sentence #640000, processed 14009938 words, keeping 505501 word types\n",
      "2016-11-09 20:49:26,309 : INFO : PROGRESS: at sentence #650000, processed 14229408 words, keeping 510812 word types\n",
      "2016-11-09 20:49:26,444 : INFO : PROGRESS: at sentence #660000, processed 14448768 words, keeping 515915 word types\n",
      "2016-11-09 20:49:26,588 : INFO : PROGRESS: at sentence #670000, processed 14671813 words, keeping 521228 word types\n",
      "2016-11-09 20:49:26,721 : INFO : PROGRESS: at sentence #680000, processed 14888172 words, keeping 526132 word types\n",
      "2016-11-09 20:49:26,853 : INFO : PROGRESS: at sentence #690000, processed 15108335 words, keeping 531319 word types\n",
      "2016-11-09 20:49:26,980 : INFO : PROGRESS: at sentence #700000, processed 15330596 words, keeping 536510 word types\n",
      "2016-11-09 20:49:27,121 : INFO : PROGRESS: at sentence #710000, processed 15553506 words, keeping 541682 word types\n",
      "2016-11-09 20:49:27,258 : INFO : PROGRESS: at sentence #720000, processed 15771658 words, keeping 546586 word types\n",
      "2016-11-09 20:49:27,383 : INFO : PROGRESS: at sentence #730000, processed 15988337 words, keeping 551384 word types\n",
      "2016-11-09 20:49:27,519 : INFO : PROGRESS: at sentence #740000, processed 16206311 words, keeping 556337 word types\n",
      "2016-11-09 20:49:27,649 : INFO : PROGRESS: at sentence #750000, processed 16421652 words, keeping 561195 word types\n",
      "2016-11-09 20:49:27,789 : INFO : PROGRESS: at sentence #760000, processed 16639516 words, keeping 566205 word types\n",
      "2016-11-09 20:49:27,933 : INFO : PROGRESS: at sentence #770000, processed 16864723 words, keeping 571193 word types\n",
      "2016-11-09 20:49:28,059 : INFO : PROGRESS: at sentence #780000, processed 17084884 words, keeping 576122 word types\n",
      "2016-11-09 20:49:28,181 : INFO : collected 580313 word types from a corpus of 17264309 raw words and 788128 sentences\n",
      "2016-11-09 20:49:28,182 : INFO : Loading a fresh vocabulary\n",
      "2016-11-09 20:49:28,930 : INFO : min_count=40 retains 20591 unique words (3% of original 580313, drops 559722)\n",
      "2016-11-09 20:49:28,931 : INFO : min_count=40 leaves 15707409 word corpus (90% of original 17264309, drops 1556900)\n",
      "2016-11-09 20:49:29,008 : INFO : deleting the raw counts dictionary of 580313 items\n",
      "2016-11-09 20:49:29,042 : INFO : sample=0.001 downsamples 46 most-common words\n",
      "2016-11-09 20:49:29,043 : INFO : downsampling leaves estimated 11695332 word corpus (74.5% of prior 15707409)\n",
      "2016-11-09 20:49:29,044 : INFO : estimated required memory for 20591 words and 300 dimensions: 59713900 bytes\n",
      "2016-11-09 20:49:29,133 : INFO : resetting layer weights\n",
      "2016-11-09 20:49:29,459 : INFO : training model with 4 workers on 20591 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2016-11-09 20:49:29,460 : INFO : expecting 788128 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-11-09 20:49:30,466 : INFO : PROGRESS: at 1.64% examples, 951822 words/s, in_qsize 6, out_qsize 0\n",
      "2016-11-09 20:49:31,469 : INFO : PROGRESS: at 3.31% examples, 957977 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-09 20:49:32,478 : INFO : PROGRESS: at 5.18% examples, 998437 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-09 20:49:33,489 : INFO : PROGRESS: at 6.99% examples, 1009639 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:49:34,496 : INFO : PROGRESS: at 8.61% examples, 997141 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-09 20:49:35,504 : INFO : PROGRESS: at 10.25% examples, 989716 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-09 20:49:36,514 : INFO : PROGRESS: at 11.88% examples, 984883 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-09 20:49:37,521 : INFO : PROGRESS: at 13.40% examples, 971803 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-09 20:49:38,526 : INFO : PROGRESS: at 14.90% examples, 960934 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-09 20:49:39,531 : INFO : PROGRESS: at 16.46% examples, 955846 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:49:40,532 : INFO : PROGRESS: at 18.03% examples, 952468 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:49:41,534 : INFO : PROGRESS: at 19.52% examples, 945723 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:49:42,540 : INFO : PROGRESS: at 21.10% examples, 943767 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:49:43,543 : INFO : PROGRESS: at 22.75% examples, 944322 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:49:44,544 : INFO : PROGRESS: at 24.37% examples, 944017 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:49:45,548 : INFO : PROGRESS: at 26.01% examples, 944740 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:49:46,551 : INFO : PROGRESS: at 27.55% examples, 941580 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:49:47,556 : INFO : PROGRESS: at 29.06% examples, 938267 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-09 20:49:48,556 : INFO : PROGRESS: at 30.51% examples, 933739 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-09 20:49:49,569 : INFO : PROGRESS: at 31.97% examples, 929695 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:49:50,570 : INFO : PROGRESS: at 33.42% examples, 925900 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-09 20:49:51,576 : INFO : PROGRESS: at 34.83% examples, 921051 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-09 20:49:52,586 : INFO : PROGRESS: at 36.24% examples, 916223 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:49:53,594 : INFO : PROGRESS: at 37.70% examples, 913523 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-09 20:49:54,599 : INFO : PROGRESS: at 39.20% examples, 911750 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-09 20:49:55,610 : INFO : PROGRESS: at 40.61% examples, 908282 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:49:56,619 : INFO : PROGRESS: at 42.01% examples, 904383 words/s, in_qsize 6, out_qsize 0\n",
      "2016-11-09 20:49:57,622 : INFO : PROGRESS: at 43.47% examples, 902234 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-09 20:49:58,628 : INFO : PROGRESS: at 44.95% examples, 900803 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-09 20:49:59,640 : INFO : PROGRESS: at 46.41% examples, 898592 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:50:00,644 : INFO : PROGRESS: at 47.79% examples, 895483 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:50:01,652 : INFO : PROGRESS: at 49.10% examples, 891403 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:50:02,653 : INFO : PROGRESS: at 50.50% examples, 889375 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-09 20:50:03,656 : INFO : PROGRESS: at 52.01% examples, 889395 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:50:04,663 : INFO : PROGRESS: at 53.56% examples, 889649 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:50:05,666 : INFO : PROGRESS: at 54.96% examples, 887612 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-09 20:50:06,671 : INFO : PROGRESS: at 56.40% examples, 886474 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-09 20:50:07,667 : INFO : PROGRESS: at 57.90% examples, 886267 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-09 20:50:08,674 : INFO : PROGRESS: at 59.47% examples, 886804 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:50:09,676 : INFO : PROGRESS: at 61.05% examples, 887894 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-09 20:50:10,681 : INFO : PROGRESS: at 62.63% examples, 888388 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:50:11,686 : INFO : PROGRESS: at 64.02% examples, 886276 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:50:12,691 : INFO : PROGRESS: at 65.25% examples, 882263 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-09 20:50:13,704 : INFO : PROGRESS: at 66.74% examples, 881597 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:50:14,710 : INFO : PROGRESS: at 68.05% examples, 879040 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:50:15,717 : INFO : PROGRESS: at 69.62% examples, 879806 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-09 20:50:16,718 : INFO : PROGRESS: at 71.20% examples, 880790 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-09 20:50:17,720 : INFO : PROGRESS: at 72.77% examples, 881640 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:50:18,735 : INFO : PROGRESS: at 74.35% examples, 882272 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:50:19,735 : INFO : PROGRESS: at 75.74% examples, 880841 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:50:20,746 : INFO : PROGRESS: at 77.30% examples, 881287 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-09 20:50:21,753 : INFO : PROGRESS: at 78.71% examples, 880216 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:50:22,755 : INFO : PROGRESS: at 80.05% examples, 878385 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:50:23,762 : INFO : PROGRESS: at 81.60% examples, 878653 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:50:24,764 : INFO : PROGRESS: at 83.10% examples, 878507 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-09 20:50:25,768 : INFO : PROGRESS: at 84.70% examples, 879311 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-09 20:50:26,771 : INFO : PROGRESS: at 86.30% examples, 880201 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-09 20:50:27,779 : INFO : PROGRESS: at 87.93% examples, 881235 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:50:28,785 : INFO : PROGRESS: at 89.35% examples, 880428 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:50:29,785 : INFO : PROGRESS: at 90.76% examples, 879504 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:50:30,788 : INFO : PROGRESS: at 92.33% examples, 880320 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-09 20:50:31,788 : INFO : PROGRESS: at 93.90% examples, 880934 words/s, in_qsize 6, out_qsize 0\n",
      "2016-11-09 20:50:32,800 : INFO : PROGRESS: at 95.25% examples, 879333 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-09 20:50:33,805 : INFO : PROGRESS: at 96.75% examples, 879186 words/s, in_qsize 8, out_qsize 0\n",
      "2016-11-09 20:50:34,809 : INFO : PROGRESS: at 98.07% examples, 877578 words/s, in_qsize 7, out_qsize 0\n",
      "2016-11-09 20:50:35,823 : INFO : PROGRESS: at 99.69% examples, 878517 words/s, in_qsize 6, out_qsize 0\n",
      "2016-11-09 20:50:36,014 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2016-11-09 20:50:36,016 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2016-11-09 20:50:36,023 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2016-11-09 20:50:36,033 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2016-11-09 20:50:36,034 : INFO : training on 86321545 raw words (58474718 effective words) took 66.6s, 878403 effective words/s\n",
      "2016-11-09 20:50:36,035 : INFO : precomputing L2-norms of word weight vectors\n",
      "2016-11-09 20:50:36,187 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2016-11-09 20:50:36,188 : INFO : not storing attribute syn0norm\n",
      "2016-11-09 20:50:36,189 : INFO : not storing attribute cum_table\n",
      "2016-11-09 20:50:36,503 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print \"Training model...\"\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'atrocious', 0.7367416620254517),\n",
       " (u'dreadful', 0.7218186855316162),\n",
       " (u'terrible', 0.7141462564468384),\n",
       " (u'horrible', 0.7044157385826111),\n",
       " (u'abysmal', 0.6916906833648682),\n",
       " (u'awful,', 0.6897714138031006),\n",
       " (u'amateurish', 0.6700375080108643),\n",
       " (u'appalling', 0.6658197045326233),\n",
       " (u'horrid', 0.6512665748596191),\n",
       " (u'embarrassing', 0.6488428115844727)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-11-09 20:53:40,118 : INFO : loading Word2Vec object from 300features_40minwords_10context\n",
      "2016-11-09 20:53:40,287 : INFO : setting ignored attribute syn0norm to None\n",
      "2016-11-09 20:53:40,288 : INFO : setting ignored attribute cum_table to None\n",
      "2016-11-09 20:53:40,288 : INFO : loaded 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load(\"300features_40minwords_10context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       #\n",
    "       # Print a status message every 1000th review\n",
    "       if counter%1000. == 0.:\n",
    "           print \"Review %d of %d\" % (counter, len(reviews))\n",
    "       # \n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
    "           num_features)\n",
    "       #\n",
    "       # Increment the counter\n",
    "       counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n",
      "Creating average feature vecs for test reviews\n",
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n"
     ]
    }
   ],
   "source": [
    "# ****************************************************************\n",
    "# Calculate average feature vectors for training and testing sets,\n",
    "# using the functions we defined above. Notice that we now use stop word\n",
    "# removal.\n",
    "\n",
    "clean_train_reviews = []\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append( review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
    "\n",
    "print \"Creating average feature vecs for test reviews\"\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append( review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-a0be0fc2979c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Test & extract results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtestDataVecs\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Write the test results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/p0a0045/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \"\"\"\n\u001b[0;32m--> 534\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/p0a0045/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \"\"\"\n\u001b[1;32m    572\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;31m# Assign chunk of trees to jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/p0a0045/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    353\u001b[0m                                  \"call `fit` before exploiting the model.\")\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/p0a0045/anaconda/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             if issparse(X) and (X.indices.dtype != np.intc or\n\u001b[1;32m    394\u001b[0m                                 X.indptr.dtype != np.intc):\n",
      "\u001b[0;32m/Users/p0a0045/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    405\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/p0a0045/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     56\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     57\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 58\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "# Fit a random forest to the training data, using 100 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier( n_estimators = 100 )\n",
    "\n",
    "print \"Fitting a random forest to labeled training data...\"\n",
    "forest = forest.fit( trainDataVecs, train[\"sentiment\"] )\n",
    "\n",
    "# Test & extract results \n",
    "result = forest.predict( testDataVecs )\n",
    "\n",
    "# Write the test results \n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "output.to_csv( \"Word2Vec_AverageVectors.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
